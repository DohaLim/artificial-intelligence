{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 신경망과 퍼셉트론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인공신경망은 사람의 뇌를 모방하여 인공지능을 구현하려는 컴퓨터 프로그램이다.\n",
    "사람의 뇌는 뉴런(neuron)으로 이루어져 있고, 이는 입력(Dendrite), 계산(Soma), 출력(Axon) 기능을 가진 정보처리기관으로 볼 수 있다. 이전 neuron의 axon과 현재 neuron의 dendrite 부분이 만나는 곳에 작은 간극인 synapse가 존재하고, 정보가 들어오면 세포체 soma에서는 들어온 정보가 역치 이상이면 발화한다. 이렇게 발화한 neuron은 axon을 통해 다음 neron에게 정보를 전달한다. \n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20230410104038/Artificial-Neural-Networks.webp\" height=\"250px\" width=\"500px\">\n",
    "\n",
    "인공신경세포(artificial neuron)는 신경세포의 정보처리 방식을 모방하여 구현한다. 그리고 이런 인공신경세포들이 모여서 인공신경망을 구성한다. 인공신경세포의 작동 원리는 뉴런으로부터 입력을 받아 역치를 계산하여 출력한다. 각각의 뉴런으로부터 오는 정보 $x_1, x_2, x_3$와 시냅스 역할을 하는 것이 연결강도 $w_1, w_2, w_3$이다. 노드에서 3개의 뉴런의 입력을 받아 합하여 노드값을 계산하는데, 이때 활성화함수를 덧붙여서 역치에 따라 발화 여부를 결정한다.\n",
    "\n",
    "퍼셉트론(Perceptron)은 1943년 신경생리학자인 McCulloch와 게산신경과학자인 Pitts가 제안한 Mcculloch-Pitts Neuron을 바탕으로 미국의 심리학자인 Rosenblatt이 1958년에 구현한 인공신경망이다.\n",
    "\n",
    "신경망의 학습은 연결가중치 weight 값을 변화시키는 것을 의미한다. 실제값($y$)과 예측값($y'$)의 차이(오차)를 최소화 하도록 퍼셉트론의 연결강도를 조절하는 것을 학습(learning)이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 다층 퍼셉트론(MLP)와 모델 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼셉트론은 2차원 평면을 분할하는 1차원 함수의 기울기와 절편을 점진적으로 찾아가는 인공신경망 모델로도 볼 수 있다. 퍼셉트론은 선형분리가 불가능한 데이터셋을 학습할 수 없다는 한계가 있어서, XOR 게이트와 같은 간단한 문제도 해결할 수 없다.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/qingkaikong/blog/master/40_ANN_part3_step_by_step_MLP/figures/figure1_Perceptron_limitation.jpg\" height=\"250px\" width=\"500px\">\n",
    "\n",
    "이를 해결하기 위해 다층 신경망(Multi-Layer Perceptron)은 하나의 입력층과 한개 이상의 은닉층(hidden layer)과 출력층으로 이루어져 있고, 층이 많아질수록 더 복잡한 형태의 데이터를 다룰 수 있다. 이러한 다층 신경망이 딥러닝(deep learning)의 첫걸음이다. 다층 신경망의 활성화 함수 또한 단층 퍼셉트론의 계단함수에서 조금 더 복잡한 시그모이드나 tanh 함수를 이용하고, 오차를 줄이기 위해 경사하강법(gradient descendent), 역전파(backpropagation) 등이 등장했다.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/354817375/figure/fig2/AS:1071622807097344@1632506195651/Multi-layer-perceptron-MLP-NN-basic-Architecture.jpg\" height=\"250px\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLP:\n",
    "  def __intit__(self, input_size, hidden_size, output_size):\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "\n",
    "    # 가중치 초기화\n",
    "    self.weight = np.random.random((self.input_size, self.hidden_size))\n",
    "\n",
    "  def sigmoid(self, x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "  def feed_forward(self, x):\n",
    "    pass\n",
    "\n",
    "  def mse_loss(self, y_true, y_pred):\n",
    "    pass\n",
    "\n",
    "  def back_propagation(self, X, y, y_pred, learning_rate):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hira",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
