{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ì¸ê³µì§€ëŠ¥ì„ í•™ìŠµí•  ë•Œ ì•Œì•„ì•¼ í•  ì£¼ìš” ëª¨ë¸ê³¼ ê¸°ìˆ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¸ê³µì§€ëŠ¥ì€ ê°€ì¥ ë„“ì€ ê°œë…ìœ¼ë¡œ ì´ë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²• ì¤‘ì— ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì´ ìˆë‹¤. ë˜í•œ, ì¸ê³µì‹ ê²½ë§ì˜ í•œ ì¢…ë¥˜ì¸ ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://hackernoon.imgix.net/images/9ln3ztv.jpg\" height=\"300px\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê³¼ê±°ì˜ ì „í†µì ì¸ í”„ë¡œê·¸ë˜ë°ì€ ë°ì´í„°ì™€ ê·œì¹™ì„ í†µí•´ ê¸°ê³„ë¡œë¶€í„° ê²°ê³¼ë¥¼ ì¶œë ¥í–ˆë‹¤ë©´, ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì™€ ì •ë‹µ ìŒì„ ê¸°ê³„ì— í•™ìŠµì‹œì¼œ ìŠ¤ìŠ¤ë¡œ ê·œì¹™ì„ ë°œê²¬í•˜ë„ë¡ í•˜ëŠ” ê¸°ìˆ ì´ë‹¤. ì´ë•Œ, ëª©í‘œëŠ” training dataë¥¼ ì™„ë²½í•˜ê²Œ í•™ìŠµí•˜ì—¬ ì¬í˜„í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ taskë¥¼ ì˜ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í†µê³„ì  ëª¨í˜•ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¨¸ì‹ ëŸ¬ë‹ì˜ ì¢…ë¥˜ëŠ” í¬ê²Œ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "- ì§€ë„ í•™ìŠµ(Supervised Learning)\n",
    "  - Training dataì— labelì´ ìˆëŠ” ê²½ìš°\n",
    "  - ex) íšŒê·€ë¶„ì„, ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„, ì˜ì‚¬ê²°ì •ë‚˜ë¬´, ëœë¤í¬ë ˆìŠ¤íŠ¸, ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  ë“±\n",
    "- ë¹„ì§€ë„ í•™ìŠµ(Unsupervised Learning)\n",
    "  -  Training dataì— labelì´ ì—†ëŠ” ê²½ìš°\n",
    "  - ex) ì£¼ì„±ë¶„ ë¶„ì„, ê³„ì¸µ í´ëŸ¬ìŠ¤í„°ë§, K-í‰ê·  í´ëŸ¬ìŠ¤í„°ë§ ë“±\n",
    "- ì¤€ì§€ë„ í•™ìŠµ(Semi-supervised Learning)\n",
    "  - ì¼ë¶€ training dataì— labelì´ ìˆëŠ” ê²½ìš°\n",
    "- ê°•í™”í•™ìŠµ(Reinforcement Learning)\n",
    "  - ê³ ì •ëœ ë°ì´í„°ì…‹ì´ ì•„ë‹Œ íŠ¹ì • í™˜ê²½ì´ ì£¼ì–´ì§„ ê²½ìš°\n",
    "  - í–‰ë™(Action)ì— ëŒ€í•œ ë³´ìƒ(Reward)ìœ¼ë¡œ í•™ìŠµì´ ì´ë£¨ì–´ì§„ë‹¤ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¨¸ì‹ ëŸ¬ë‹ì˜ ëª¨í˜• ë° ì£¼ìš” ê¸°ìˆ ì—ëŠ” ì‹œê³„ì—´ ë¶„ì„(ì¶”ì„¸ ë¶„ì„, ë¯¸ë˜ ìˆ˜ìš” ì˜ˆì¸¡, ìë™ ë¬¸ì¥ ìƒì„± ë“±), OCR(ì´ë¯¸ì§€ ì† í…ìŠ¤íŠ¸ ì¶”ì¶œ), ì´ë¯¸ì§€ ë¶„ë¥˜, ì¶”ì²œ, ì˜ìƒ ì²˜ë¦¬, ìì—°ì–´ ì²˜ë¦¬, ì±—ë´‡, ìŒì„± ì¸ì‹ ë“± ê·¸ ì¢…ë¥˜ê°€ ë‹¤ì–‘í•˜ë‹¤. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ë¨¸ì‹ ëŸ¬ë‹ì˜ ì§€ë„í•™ìŠµì˜ ëŒ€í‘œì ì¸ ëª¨ë¸ê³¼ ê¸°ìˆ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(ğ’™^1, ğ‘¦^1) , â€¦ , (ğ’™^ğ‘, ğ‘¦^ğ‘)$ì˜ ë ˆì´ë¸”ë§ ëœ ìƒ˜í”Œì´ ìˆì„ ë•Œ, ë§¤í•‘ í•¨ìˆ˜ $ğ‘”: ğ‘¿ â†’ ğ’€$ë¥¼ í•™ìŠµí•´ì„œ ìƒˆë¡œìš´ $ğ’™'$ê°€ ì£¼ì–´ì§ˆ ë•Œ ì í•©í•œ $ğ‘¦'$ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ì¶œë ¥í•˜ëŠ” ê²ƒì„ ì§€ë„í•™ìŠµ(Supervised Learning)ì´ë¼ê³  í•œë‹¤. ì´ë•Œ, ì§€ë„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ output $y$ê°€ continuous ë˜ëŠ” discreteí•¨ì— ë”°ë¼ íšŒê·€(Regression)ì™€ ë¶„ë¥˜(Classification) í¬ê²Œ ë‘ê°€ì§€ ìœ í˜•ìœ¼ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.simplilearn.com/ice9/free_resources_article_thumb/Regression_vs_Classification.jpg\" height=\"250px\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì§€ë„í•™ìŠµì˜ ëŒ€í‘œì ì¸ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "\n",
    "- ì„ í˜• íšŒê·€ë¶„ì„(Linear Regression): ì¢…ì† ë³€ìˆ˜ì™€ í•˜ë‚˜ ì´ìƒì˜ ë…ë¦½ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” ì„ í˜•ì‹ì„ ì°¾ëŠ” ëª¨ë¸ì´ë‹¤. ì´ë•Œ, ë…ë¦½ ë³€ìˆ˜ì˜ ê°œìˆ˜ê°€ í•˜ë‚˜ì¼ ë•Œë¥¼ ë‹¨ìˆœ ì„ í˜• íšŒê·€, ì—¬ëŸ¬ê°œì¸ ê²½ìš°ì— ë‹¤ì¤‘ ì„ í˜• íšŒê·€ë¼ê³  í•œë‹¤.\n",
    "\n",
    "  <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20231129130431/11111111.png\" height=\"200px\" width=\"300px\">\n",
    "\n",
    "- ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„(Logistic Regression): ì„ í˜• íšŒê·€ë¶„ì„ì€ ì¶œë ¥ë³€ìˆ˜ê°€ ì—°ì†ì ì¸ ê°’ì—ë§Œ ê°€ëŠ¥í•œ ë°˜ë©´, ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ì€ ë²”ì£¼í˜•ë„ ê°€ëŠ¥í•˜ë‹¤.\n",
    "\n",
    "  <img src=\"https://www.researchgate.net/publication/367007823/figure/fig1/AS:11431281112303217@1673379396543/Logistic-regression-of-binary-classification-353-Support-Vector-Machine-SVM.ppm\" height=\"240px\" width=\"300px\">\n",
    "\n",
    "- K-ìµœê·¼ì ‘ ì´ì›ƒ(KNN, K-nearest Neighbors): ê¸°ë³¸ì ìœ¼ë¡œ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ë¥¼ í†µí•´ Kê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ ì¤‘ ê°€ì¥ ê°€ê¹Œì´ ìˆëŠ” í¬ì¸íŠ¸ì˜ ë¼ë²¨ì— ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸\n",
    "\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:505/0*2_qzcm2gSe9l67aI.png\" height=\"250px\" width=\"300px\">\n",
    "  \n",
    "- ê²°ì • íŠ¸ë¦¬(Decision Tree Classification): íŠ¹ì • ê¸°ì¤€ì— ë”°ë¼ ë°ì´í„°ë¥¼ êµ¬ë¶„í•´ì„œ ì¼ë ¨ì˜ ë¶„ë¥˜ ê·œì¹™ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ì´ë‹¤.\n",
    "\n",
    "  <img src=\"https://www.mastersindatascience.org/wp-content/uploads/sites/54/2022/05/tree-graphic.jpg\" height=\"200px\" width=\"300px\">\n",
    "\n",
    "- ëœë¤ í¬ë ˆìŠ¤íŠ¸(Random Forest Classification): ì—¬ëŸ¬ ê°œì˜ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ë¥¼ êµ¬ì„±í•˜ê³ , ê° ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë‹¤ìˆ˜ê²° ë°©ì‹ìœ¼ë¡œ ì¡°í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê²°ì •í•˜ëŠ” ëª¨ë¸ì´ë‹¤.\n",
    "\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1010/1*R3oJiyaQwyLUyLZL-scDpw.png\" height=\"200px\" width=\"300px\">\n",
    "\n",
    "- ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ (SVM, Support Vector Machine): ì„œí¬íŠ¸ ë²¡í„°(ê²½ê³„ì„ ) ê°„ì˜ ë„ˆë¹„ë¥¼ ìµœëŒ€í™”, ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ë©°, ê³ ì°¨ì› ë°ì´í„°ì…‹ì„ ë¶„ë¥˜í•˜ëŠ”ë° ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.\n",
    "\n",
    "  <img src=\"https://3.bp.blogspot.com/-12I3KUZYAZU/WHI90_mZokI/AAAAAAAAFzg/qaaiCYvhwT41_rp0PEQjE7GFkPEtNrzkwCLcB/s1600-rw/SVM%2Bin%2BR.png\" height=\"200px\" width=\"300px\">\n",
    "\n",
    "- ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ(Naive Bayes Classifier): Featureë¼ë¦¬ ì¡°ê±´ë¶€ ë…ë¦½ì´ë¼ëŠ” ê°€ì •í•˜ì— ë¶„ë¥˜ê°€ ì´ë£¨ì–´ì§€ë©°, ì„¤ëª…í•˜ê¸° ì‰½ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scikit-learn ë¶„ë¥˜ ë˜ëŠ” íšŒê·€ ì•Œê³ ë¦¬ì¦˜ ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      "|details-start|\n",
      "**References**\n",
      "|details-split|\n",
      "\n",
      "(1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "Comparison of Classifiers in High Dimensional Settings, \n",
      "Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "Mathematics and Statistics, James Cook University of North Queensland. \n",
      "(Also submitted to Technometrics). \n",
      "\n",
      "The data was used with many others for comparing various \n",
      "classifiers. The classes are separable, though only RDA \n",
      "has achieved 100% correct classification. \n",
      "(RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "(All results using the leave-one-out technique) \n",
      "\n",
      "(2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "\"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "Mathematics and Statistics, James Cook University of North Queensland. \n",
      "(Also submitted to Journal of Chemometrics).\n",
      "\n",
      "|details-end|\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# dataset -> dataframe\n",
    "wine = load_wine()\n",
    "print(wine.DESCR) # the full description of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      " 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03] 0\n",
      "2 [1.32e+01 1.78e+00 2.14e+00 1.12e+01 1.00e+02 2.65e+00 2.76e+00 2.60e-01\n",
      " 1.28e+00 4.38e+00 1.05e+00 3.40e+00 1.05e+03] 0\n",
      "3 [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00\n",
      " 3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03] 0\n",
      "4 [1.437e+01 1.950e+00 2.500e+00 1.680e+01 1.130e+02 3.850e+00 3.490e+00\n",
      " 2.400e-01 2.180e+00 7.800e+00 8.600e-01 3.450e+00 1.480e+03] 0\n",
      "5 [1.324e+01 2.590e+00 2.870e+00 2.100e+01 1.180e+02 2.800e+00 2.690e+00\n",
      " 3.900e-01 1.820e+00 4.320e+00 1.040e+00 2.930e+00 7.350e+02] 0\n",
      "6 [1.42e+01 1.76e+00 2.45e+00 1.52e+01 1.12e+02 3.27e+00 3.39e+00 3.40e-01\n",
      " 1.97e+00 6.75e+00 1.05e+00 2.85e+00 1.45e+03] 0\n",
      "7 [1.439e+01 1.870e+00 2.450e+00 1.460e+01 9.600e+01 2.500e+00 2.520e+00\n",
      " 3.000e-01 1.980e+00 5.250e+00 1.020e+00 3.580e+00 1.290e+03] 0\n",
      "8 [1.406e+01 2.150e+00 2.610e+00 1.760e+01 1.210e+02 2.600e+00 2.510e+00\n",
      " 3.100e-01 1.250e+00 5.050e+00 1.060e+00 3.580e+00 1.295e+03] 0\n",
      "9 [1.483e+01 1.640e+00 2.170e+00 1.400e+01 9.700e+01 2.800e+00 2.980e+00\n",
      " 2.900e-01 1.980e+00 5.200e+00 1.080e+00 2.850e+00 1.045e+03] 0\n",
      "10 [1.386e+01 1.350e+00 2.270e+00 1.600e+01 9.800e+01 2.980e+00 3.150e+00\n",
      " 2.200e-01 1.850e+00 7.220e+00 1.010e+00 3.550e+00 1.045e+03] 0\n",
      "11 [1.41e+01 2.16e+00 2.30e+00 1.80e+01 1.05e+02 2.95e+00 3.32e+00 2.20e-01\n",
      " 2.38e+00 5.75e+00 1.25e+00 3.17e+00 1.51e+03] 0\n",
      "12 [1.412e+01 1.480e+00 2.320e+00 1.680e+01 9.500e+01 2.200e+00 2.430e+00\n",
      " 2.600e-01 1.570e+00 5.000e+00 1.170e+00 2.820e+00 1.280e+03] 0\n",
      "13 [1.375e+01 1.730e+00 2.410e+00 1.600e+01 8.900e+01 2.600e+00 2.760e+00\n",
      " 2.900e-01 1.810e+00 5.600e+00 1.150e+00 2.900e+00 1.320e+03] 0\n",
      "14 [1.475e+01 1.730e+00 2.390e+00 1.140e+01 9.100e+01 3.100e+00 3.690e+00\n",
      " 4.300e-01 2.810e+00 5.400e+00 1.250e+00 2.730e+00 1.150e+03] 0\n",
      "15 [1.438e+01 1.870e+00 2.380e+00 1.200e+01 1.020e+02 3.300e+00 3.640e+00\n",
      " 2.900e-01 2.960e+00 7.500e+00 1.200e+00 3.000e+00 1.547e+03] 0\n",
      "16 [1.363e+01 1.810e+00 2.700e+00 1.720e+01 1.120e+02 2.850e+00 2.910e+00\n",
      " 3.000e-01 1.460e+00 7.300e+00 1.280e+00 2.880e+00 1.310e+03] 0\n",
      "17 [1.43e+01 1.92e+00 2.72e+00 2.00e+01 1.20e+02 2.80e+00 3.14e+00 3.30e-01\n",
      " 1.97e+00 6.20e+00 1.07e+00 2.65e+00 1.28e+03] 0\n",
      "18 [1.383e+01 1.570e+00 2.620e+00 2.000e+01 1.150e+02 2.950e+00 3.400e+00\n",
      " 4.000e-01 1.720e+00 6.600e+00 1.130e+00 2.570e+00 1.130e+03] 0\n",
      "19 [1.419e+01 1.590e+00 2.480e+00 1.650e+01 1.080e+02 3.300e+00 3.930e+00\n",
      " 3.200e-01 1.860e+00 8.700e+00 1.230e+00 2.820e+00 1.680e+03] 0\n",
      "20 [1.364e+01 3.100e+00 2.560e+00 1.520e+01 1.160e+02 2.700e+00 3.030e+00\n",
      " 1.700e-01 1.660e+00 5.100e+00 9.600e-01 3.360e+00 8.450e+02] 0\n",
      "21 [1.406e+01 1.630e+00 2.280e+00 1.600e+01 1.260e+02 3.000e+00 3.170e+00\n",
      " 2.400e-01 2.100e+00 5.650e+00 1.090e+00 3.710e+00 7.800e+02] 0\n",
      "22 [1.293e+01 3.800e+00 2.650e+00 1.860e+01 1.020e+02 2.410e+00 2.410e+00\n",
      " 2.500e-01 1.980e+00 4.500e+00 1.030e+00 3.520e+00 7.700e+02] 0\n",
      "23 [1.371e+01 1.860e+00 2.360e+00 1.660e+01 1.010e+02 2.610e+00 2.880e+00\n",
      " 2.700e-01 1.690e+00 3.800e+00 1.110e+00 4.000e+00 1.035e+03] 0\n",
      "24 [1.285e+01 1.600e+00 2.520e+00 1.780e+01 9.500e+01 2.480e+00 2.370e+00\n",
      " 2.600e-01 1.460e+00 3.930e+00 1.090e+00 3.630e+00 1.015e+03] 0\n",
      "25 [1.35e+01 1.81e+00 2.61e+00 2.00e+01 9.60e+01 2.53e+00 2.61e+00 2.80e-01\n",
      " 1.66e+00 3.52e+00 1.12e+00 3.82e+00 8.45e+02] 0\n",
      "26 [1.305e+01 2.050e+00 3.220e+00 2.500e+01 1.240e+02 2.630e+00 2.680e+00\n",
      " 4.700e-01 1.920e+00 3.580e+00 1.130e+00 3.200e+00 8.300e+02] 0\n",
      "27 [1.339e+01 1.770e+00 2.620e+00 1.610e+01 9.300e+01 2.850e+00 2.940e+00\n",
      " 3.400e-01 1.450e+00 4.800e+00 9.200e-01 3.220e+00 1.195e+03] 0\n",
      "28 [1.330e+01 1.720e+00 2.140e+00 1.700e+01 9.400e+01 2.400e+00 2.190e+00\n",
      " 2.700e-01 1.350e+00 3.950e+00 1.020e+00 2.770e+00 1.285e+03] 0\n",
      "29 [1.387e+01 1.900e+00 2.800e+00 1.940e+01 1.070e+02 2.950e+00 2.970e+00\n",
      " 3.700e-01 1.760e+00 4.500e+00 1.250e+00 3.400e+00 9.150e+02] 0\n",
      "30 [1.402e+01 1.680e+00 2.210e+00 1.600e+01 9.600e+01 2.650e+00 2.330e+00\n",
      " 2.600e-01 1.980e+00 4.700e+00 1.040e+00 3.590e+00 1.035e+03] 0\n",
      "31 [1.373e+01 1.500e+00 2.700e+00 2.250e+01 1.010e+02 3.000e+00 3.250e+00\n",
      " 2.900e-01 2.380e+00 5.700e+00 1.190e+00 2.710e+00 1.285e+03] 0\n",
      "32 [1.358e+01 1.660e+00 2.360e+00 1.910e+01 1.060e+02 2.860e+00 3.190e+00\n",
      " 2.200e-01 1.950e+00 6.900e+00 1.090e+00 2.880e+00 1.515e+03] 0\n",
      "33 [1.368e+01 1.830e+00 2.360e+00 1.720e+01 1.040e+02 2.420e+00 2.690e+00\n",
      " 4.200e-01 1.970e+00 3.840e+00 1.230e+00 2.870e+00 9.900e+02] 0\n",
      "34 [1.376e+01 1.530e+00 2.700e+00 1.950e+01 1.320e+02 2.950e+00 2.740e+00\n",
      " 5.000e-01 1.350e+00 5.400e+00 1.250e+00 3.000e+00 1.235e+03] 0\n",
      "35 [1.351e+01 1.800e+00 2.650e+00 1.900e+01 1.100e+02 2.350e+00 2.530e+00\n",
      " 2.900e-01 1.540e+00 4.200e+00 1.100e+00 2.870e+00 1.095e+03] 0\n",
      "36 [1.348e+01 1.810e+00 2.410e+00 2.050e+01 1.000e+02 2.700e+00 2.980e+00\n",
      " 2.600e-01 1.860e+00 5.100e+00 1.040e+00 3.470e+00 9.200e+02] 0\n",
      "37 [1.328e+01 1.640e+00 2.840e+00 1.550e+01 1.100e+02 2.600e+00 2.680e+00\n",
      " 3.400e-01 1.360e+00 4.600e+00 1.090e+00 2.780e+00 8.800e+02] 0\n",
      "38 [1.305e+01 1.650e+00 2.550e+00 1.800e+01 9.800e+01 2.450e+00 2.430e+00\n",
      " 2.900e-01 1.440e+00 4.250e+00 1.120e+00 2.510e+00 1.105e+03] 0\n",
      "39 [1.307e+01 1.500e+00 2.100e+00 1.550e+01 9.800e+01 2.400e+00 2.640e+00\n",
      " 2.800e-01 1.370e+00 3.700e+00 1.180e+00 2.690e+00 1.020e+03] 0\n",
      "40 [1.422e+01 3.990e+00 2.510e+00 1.320e+01 1.280e+02 3.000e+00 3.040e+00\n",
      " 2.000e-01 2.080e+00 5.100e+00 8.900e-01 3.530e+00 7.600e+02] 0\n",
      "41 [1.356e+01 1.710e+00 2.310e+00 1.620e+01 1.170e+02 3.150e+00 3.290e+00\n",
      " 3.400e-01 2.340e+00 6.130e+00 9.500e-01 3.380e+00 7.950e+02] 0\n",
      "42 [1.341e+01 3.840e+00 2.120e+00 1.880e+01 9.000e+01 2.450e+00 2.680e+00\n",
      " 2.700e-01 1.480e+00 4.280e+00 9.100e-01 3.000e+00 1.035e+03] 0\n",
      "43 [1.388e+01 1.890e+00 2.590e+00 1.500e+01 1.010e+02 3.250e+00 3.560e+00\n",
      " 1.700e-01 1.700e+00 5.430e+00 8.800e-01 3.560e+00 1.095e+03] 0\n",
      "44 [1.324e+01 3.980e+00 2.290e+00 1.750e+01 1.030e+02 2.640e+00 2.630e+00\n",
      " 3.200e-01 1.660e+00 4.360e+00 8.200e-01 3.000e+00 6.800e+02] 0\n",
      "45 [1.305e+01 1.770e+00 2.100e+00 1.700e+01 1.070e+02 3.000e+00 3.000e+00\n",
      " 2.800e-01 2.030e+00 5.040e+00 8.800e-01 3.350e+00 8.850e+02] 0\n",
      "46 [1.421e+01 4.040e+00 2.440e+00 1.890e+01 1.110e+02 2.850e+00 2.650e+00\n",
      " 3.000e-01 1.250e+00 5.240e+00 8.700e-01 3.330e+00 1.080e+03] 0\n",
      "47 [1.438e+01 3.590e+00 2.280e+00 1.600e+01 1.020e+02 3.250e+00 3.170e+00\n",
      " 2.700e-01 2.190e+00 4.900e+00 1.040e+00 3.440e+00 1.065e+03] 0\n",
      "48 [1.39e+01 1.68e+00 2.12e+00 1.60e+01 1.01e+02 3.10e+00 3.39e+00 2.10e-01\n",
      " 2.14e+00 6.10e+00 9.10e-01 3.33e+00 9.85e+02] 0\n",
      "49 [1.41e+01 2.02e+00 2.40e+00 1.88e+01 1.03e+02 2.75e+00 2.92e+00 3.20e-01\n",
      " 2.38e+00 6.20e+00 1.07e+00 2.75e+00 1.06e+03] 0\n",
      "50 [1.394e+01 1.730e+00 2.270e+00 1.740e+01 1.080e+02 2.880e+00 3.540e+00\n",
      " 3.200e-01 2.080e+00 8.900e+00 1.120e+00 3.100e+00 1.260e+03] 0\n",
      "51 [1.305e+01 1.730e+00 2.040e+00 1.240e+01 9.200e+01 2.720e+00 3.270e+00\n",
      " 1.700e-01 2.910e+00 7.200e+00 1.120e+00 2.910e+00 1.150e+03] 0\n",
      "52 [1.383e+01 1.650e+00 2.600e+00 1.720e+01 9.400e+01 2.450e+00 2.990e+00\n",
      " 2.200e-01 2.290e+00 5.600e+00 1.240e+00 3.370e+00 1.265e+03] 0\n",
      "53 [1.382e+01 1.750e+00 2.420e+00 1.400e+01 1.110e+02 3.880e+00 3.740e+00\n",
      " 3.200e-01 1.870e+00 7.050e+00 1.010e+00 3.260e+00 1.190e+03] 0\n",
      "54 [1.377e+01 1.900e+00 2.680e+00 1.710e+01 1.150e+02 3.000e+00 2.790e+00\n",
      " 3.900e-01 1.680e+00 6.300e+00 1.130e+00 2.930e+00 1.375e+03] 0\n",
      "55 [1.374e+01 1.670e+00 2.250e+00 1.640e+01 1.180e+02 2.600e+00 2.900e+00\n",
      " 2.100e-01 1.620e+00 5.850e+00 9.200e-01 3.200e+00 1.060e+03] 0\n",
      "56 [1.356e+01 1.730e+00 2.460e+00 2.050e+01 1.160e+02 2.960e+00 2.780e+00\n",
      " 2.000e-01 2.450e+00 6.250e+00 9.800e-01 3.030e+00 1.120e+03] 0\n",
      "57 [1.422e+01 1.700e+00 2.300e+00 1.630e+01 1.180e+02 3.200e+00 3.000e+00\n",
      " 2.600e-01 2.030e+00 6.380e+00 9.400e-01 3.310e+00 9.700e+02] 0\n",
      "58 [1.329e+01 1.970e+00 2.680e+00 1.680e+01 1.020e+02 3.000e+00 3.230e+00\n",
      " 3.100e-01 1.660e+00 6.000e+00 1.070e+00 2.840e+00 1.270e+03] 0\n",
      "59 [1.372e+01 1.430e+00 2.500e+00 1.670e+01 1.080e+02 3.400e+00 3.670e+00\n",
      " 1.900e-01 2.040e+00 6.800e+00 8.900e-01 2.870e+00 1.285e+03] 0\n",
      "60 [1.237e+01 9.400e-01 1.360e+00 1.060e+01 8.800e+01 1.980e+00 5.700e-01\n",
      " 2.800e-01 4.200e-01 1.950e+00 1.050e+00 1.820e+00 5.200e+02] 1\n",
      "61 [1.233e+01 1.100e+00 2.280e+00 1.600e+01 1.010e+02 2.050e+00 1.090e+00\n",
      " 6.300e-01 4.100e-01 3.270e+00 1.250e+00 1.670e+00 6.800e+02] 1\n",
      "62 [ 12.64   1.36   2.02  16.8  100.     2.02   1.41   0.53   0.62   5.75\n",
      "   0.98   1.59 450.  ] 1\n",
      "63 [1.367e+01 1.250e+00 1.920e+00 1.800e+01 9.400e+01 2.100e+00 1.790e+00\n",
      " 3.200e-01 7.300e-01 3.800e+00 1.230e+00 2.460e+00 6.300e+02] 1\n",
      "64 [1.237e+01 1.130e+00 2.160e+00 1.900e+01 8.700e+01 3.500e+00 3.100e+00\n",
      " 1.900e-01 1.870e+00 4.450e+00 1.220e+00 2.870e+00 4.200e+02] 1\n",
      "65 [ 12.17   1.45   2.53  19.   104.     1.89   1.75   0.45   1.03   2.95\n",
      "   1.45   2.23 355.  ] 1\n",
      "66 [1.237e+01 1.210e+00 2.560e+00 1.810e+01 9.800e+01 2.420e+00 2.650e+00\n",
      " 3.700e-01 2.080e+00 4.600e+00 1.190e+00 2.300e+00 6.780e+02] 1\n",
      "67 [1.311e+01 1.010e+00 1.700e+00 1.500e+01 7.800e+01 2.980e+00 3.180e+00\n",
      " 2.600e-01 2.280e+00 5.300e+00 1.120e+00 3.180e+00 5.020e+02] 1\n",
      "68 [1.237e+01 1.170e+00 1.920e+00 1.960e+01 7.800e+01 2.110e+00 2.000e+00\n",
      " 2.700e-01 1.040e+00 4.680e+00 1.120e+00 3.480e+00 5.100e+02] 1\n",
      "69 [1.334e+01 9.400e-01 2.360e+00 1.700e+01 1.100e+02 2.530e+00 1.300e+00\n",
      " 5.500e-01 4.200e-01 3.170e+00 1.020e+00 1.930e+00 7.500e+02] 1\n",
      "70 [1.221e+01 1.190e+00 1.750e+00 1.680e+01 1.510e+02 1.850e+00 1.280e+00\n",
      " 1.400e-01 2.500e+00 2.850e+00 1.280e+00 3.070e+00 7.180e+02] 1\n",
      "71 [1.229e+01 1.610e+00 2.210e+00 2.040e+01 1.030e+02 1.100e+00 1.020e+00\n",
      " 3.700e-01 1.460e+00 3.050e+00 9.060e-01 1.820e+00 8.700e+02] 1\n",
      "72 [1.386e+01 1.510e+00 2.670e+00 2.500e+01 8.600e+01 2.950e+00 2.860e+00\n",
      " 2.100e-01 1.870e+00 3.380e+00 1.360e+00 3.160e+00 4.100e+02] 1\n",
      "73 [1.349e+01 1.660e+00 2.240e+00 2.400e+01 8.700e+01 1.880e+00 1.840e+00\n",
      " 2.700e-01 1.030e+00 3.740e+00 9.800e-01 2.780e+00 4.720e+02] 1\n",
      "74 [1.299e+01 1.670e+00 2.600e+00 3.000e+01 1.390e+02 3.300e+00 2.890e+00\n",
      " 2.100e-01 1.960e+00 3.350e+00 1.310e+00 3.500e+00 9.850e+02] 1\n",
      "75 [1.196e+01 1.090e+00 2.300e+00 2.100e+01 1.010e+02 3.380e+00 2.140e+00\n",
      " 1.300e-01 1.650e+00 3.210e+00 9.900e-01 3.130e+00 8.860e+02] 1\n",
      "76 [1.166e+01 1.880e+00 1.920e+00 1.600e+01 9.700e+01 1.610e+00 1.570e+00\n",
      " 3.400e-01 1.150e+00 3.800e+00 1.230e+00 2.140e+00 4.280e+02] 1\n",
      "77 [1.303e+01 9.000e-01 1.710e+00 1.600e+01 8.600e+01 1.950e+00 2.030e+00\n",
      " 2.400e-01 1.460e+00 4.600e+00 1.190e+00 2.480e+00 3.920e+02] 1\n",
      "78 [1.184e+01 2.890e+00 2.230e+00 1.800e+01 1.120e+02 1.720e+00 1.320e+00\n",
      " 4.300e-01 9.500e-01 2.650e+00 9.600e-01 2.520e+00 5.000e+02] 1\n",
      "79 [1.233e+01 9.900e-01 1.950e+00 1.480e+01 1.360e+02 1.900e+00 1.850e+00\n",
      " 3.500e-01 2.760e+00 3.400e+00 1.060e+00 2.310e+00 7.500e+02] 1\n",
      "80 [1.27e+01 3.87e+00 2.40e+00 2.30e+01 1.01e+02 2.83e+00 2.55e+00 4.30e-01\n",
      " 1.95e+00 2.57e+00 1.19e+00 3.13e+00 4.63e+02] 1\n",
      "81 [ 12.     0.92   2.    19.    86.     2.42   2.26   0.3    1.43   2.5\n",
      "   1.38   3.12 278.  ] 1\n",
      "82 [1.272e+01 1.810e+00 2.200e+00 1.880e+01 8.600e+01 2.200e+00 2.530e+00\n",
      " 2.600e-01 1.770e+00 3.900e+00 1.160e+00 3.140e+00 7.140e+02] 1\n",
      "83 [1.208e+01 1.130e+00 2.510e+00 2.400e+01 7.800e+01 2.000e+00 1.580e+00\n",
      " 4.000e-01 1.400e+00 2.200e+00 1.310e+00 2.720e+00 6.300e+02] 1\n",
      "84 [ 13.05   3.86   2.32  22.5   85.     1.65   1.59   0.61   1.62   4.8\n",
      "   0.84   2.01 515.  ] 1\n",
      "85 [1.184e+01 8.900e-01 2.580e+00 1.800e+01 9.400e+01 2.200e+00 2.210e+00\n",
      " 2.200e-01 2.350e+00 3.050e+00 7.900e-01 3.080e+00 5.200e+02] 1\n",
      "86 [1.267e+01 9.800e-01 2.240e+00 1.800e+01 9.900e+01 2.200e+00 1.940e+00\n",
      " 3.000e-01 1.460e+00 2.620e+00 1.230e+00 3.160e+00 4.500e+02] 1\n",
      "87 [1.216e+01 1.610e+00 2.310e+00 2.280e+01 9.000e+01 1.780e+00 1.690e+00\n",
      " 4.300e-01 1.560e+00 2.450e+00 1.330e+00 2.260e+00 4.950e+02] 1\n",
      "88 [1.165e+01 1.670e+00 2.620e+00 2.600e+01 8.800e+01 1.920e+00 1.610e+00\n",
      " 4.000e-01 1.340e+00 2.600e+00 1.360e+00 3.210e+00 5.620e+02] 1\n",
      "89 [1.164e+01 2.060e+00 2.460e+00 2.160e+01 8.400e+01 1.950e+00 1.690e+00\n",
      " 4.800e-01 1.350e+00 2.800e+00 1.000e+00 2.750e+00 6.800e+02] 1\n",
      "90 [1.208e+01 1.330e+00 2.300e+00 2.360e+01 7.000e+01 2.200e+00 1.590e+00\n",
      " 4.200e-01 1.380e+00 1.740e+00 1.070e+00 3.210e+00 6.250e+02] 1\n",
      "91 [ 12.08   1.83   2.32  18.5   81.     1.6    1.5    0.52   1.64   2.4\n",
      "   1.08   2.27 480.  ] 1\n",
      "92 [ 12.     1.51   2.42  22.    86.     1.45   1.25   0.5    1.63   3.6\n",
      "   1.05   2.65 450.  ] 1\n",
      "93 [ 12.69   1.53   2.26  20.7   80.     1.38   1.46   0.58   1.62   3.05\n",
      "   0.96   2.06 495.  ] 1\n",
      "94 [1.229e+01 2.830e+00 2.220e+00 1.800e+01 8.800e+01 2.450e+00 2.250e+00\n",
      " 2.500e-01 1.990e+00 2.150e+00 1.150e+00 3.300e+00 2.900e+02] 1\n",
      "95 [1.162e+01 1.990e+00 2.280e+00 1.800e+01 9.800e+01 3.020e+00 2.260e+00\n",
      " 1.700e-01 1.350e+00 3.250e+00 1.160e+00 2.960e+00 3.450e+02] 1\n",
      "96 [1.247e+01 1.520e+00 2.200e+00 1.900e+01 1.620e+02 2.500e+00 2.270e+00\n",
      " 3.200e-01 3.280e+00 2.600e+00 1.160e+00 2.630e+00 9.370e+02] 1\n",
      "97 [1.181e+01 2.120e+00 2.740e+00 2.150e+01 1.340e+02 1.600e+00 9.900e-01\n",
      " 1.400e-01 1.560e+00 2.500e+00 9.500e-01 2.260e+00 6.250e+02] 1\n",
      "98 [1.229e+01 1.410e+00 1.980e+00 1.600e+01 8.500e+01 2.550e+00 2.500e+00\n",
      " 2.900e-01 1.770e+00 2.900e+00 1.230e+00 2.740e+00 4.280e+02] 1\n",
      "99 [1.237e+01 1.070e+00 2.100e+00 1.850e+01 8.800e+01 3.520e+00 3.750e+00\n",
      " 2.400e-01 1.950e+00 4.500e+00 1.040e+00 2.770e+00 6.600e+02] 1\n",
      "100 [ 12.29   3.17   2.21  18.    88.     2.85   2.99   0.45   2.81   2.3\n",
      "   1.42   2.83 406.  ] 1\n",
      "101 [1.208e+01 2.080e+00 1.700e+00 1.750e+01 9.700e+01 2.230e+00 2.170e+00\n",
      " 2.600e-01 1.400e+00 3.300e+00 1.270e+00 2.960e+00 7.100e+02] 1\n",
      "102 [1.26e+01 1.34e+00 1.90e+00 1.85e+01 8.80e+01 1.45e+00 1.36e+00 2.90e-01\n",
      " 1.35e+00 2.45e+00 1.04e+00 2.77e+00 5.62e+02] 1\n",
      "103 [1.234e+01 2.450e+00 2.460e+00 2.100e+01 9.800e+01 2.560e+00 2.110e+00\n",
      " 3.400e-01 1.310e+00 2.800e+00 8.000e-01 3.380e+00 4.380e+02] 1\n",
      "104 [1.182e+01 1.720e+00 1.880e+00 1.950e+01 8.600e+01 2.500e+00 1.640e+00\n",
      " 3.700e-01 1.420e+00 2.060e+00 9.400e-01 2.440e+00 4.150e+02] 1\n",
      "105 [1.251e+01 1.730e+00 1.980e+00 2.050e+01 8.500e+01 2.200e+00 1.920e+00\n",
      " 3.200e-01 1.480e+00 2.940e+00 1.040e+00 3.570e+00 6.720e+02] 1\n",
      "106 [ 12.42   2.55   2.27  22.    90.     1.68   1.84   0.66   1.42   2.7\n",
      "   0.86   3.3  315.  ] 1\n",
      "107 [1.225e+01 1.730e+00 2.120e+00 1.900e+01 8.000e+01 1.650e+00 2.030e+00\n",
      " 3.700e-01 1.630e+00 3.400e+00 1.000e+00 3.170e+00 5.100e+02] 1\n",
      "108 [1.272e+01 1.750e+00 2.280e+00 2.250e+01 8.400e+01 1.380e+00 1.760e+00\n",
      " 4.800e-01 1.630e+00 3.300e+00 8.800e-01 2.420e+00 4.880e+02] 1\n",
      "109 [ 12.22   1.29   1.94  19.    92.     2.36   2.04   0.39   2.08   2.7\n",
      "   0.86   3.02 312.  ] 1\n",
      "110 [1.161e+01 1.350e+00 2.700e+00 2.000e+01 9.400e+01 2.740e+00 2.920e+00\n",
      " 2.900e-01 2.490e+00 2.650e+00 9.600e-01 3.260e+00 6.800e+02] 1\n",
      "111 [1.146e+01 3.740e+00 1.820e+00 1.950e+01 1.070e+02 3.180e+00 2.580e+00\n",
      " 2.400e-01 3.580e+00 2.900e+00 7.500e-01 2.810e+00 5.620e+02] 1\n",
      "112 [1.252e+01 2.430e+00 2.170e+00 2.100e+01 8.800e+01 2.550e+00 2.270e+00\n",
      " 2.600e-01 1.220e+00 2.000e+00 9.000e-01 2.780e+00 3.250e+02] 1\n",
      "113 [1.176e+01 2.680e+00 2.920e+00 2.000e+01 1.030e+02 1.750e+00 2.030e+00\n",
      " 6.000e-01 1.050e+00 3.800e+00 1.230e+00 2.500e+00 6.070e+02] 1\n",
      "114 [1.141e+01 7.400e-01 2.500e+00 2.100e+01 8.800e+01 2.480e+00 2.010e+00\n",
      " 4.200e-01 1.440e+00 3.080e+00 1.100e+00 2.310e+00 4.340e+02] 1\n",
      "115 [ 12.08   1.39   2.5   22.5   84.     2.56   2.29   0.43   1.04   2.9\n",
      "   0.93   3.19 385.  ] 1\n",
      "116 [ 11.03   1.51   2.2   21.5   85.     2.46   2.17   0.52   2.01   1.9\n",
      "   1.71   2.87 407.  ] 1\n",
      "117 [1.182e+01 1.470e+00 1.990e+00 2.080e+01 8.600e+01 1.980e+00 1.600e+00\n",
      " 3.000e-01 1.530e+00 1.950e+00 9.500e-01 3.330e+00 4.950e+02] 1\n",
      "118 [1.242e+01 1.610e+00 2.190e+00 2.250e+01 1.080e+02 2.000e+00 2.090e+00\n",
      " 3.400e-01 1.610e+00 2.060e+00 1.060e+00 2.960e+00 3.450e+02] 1\n",
      "119 [ 12.77   3.43   1.98  16.    80.     1.63   1.25   0.43   0.83   3.4\n",
      "   0.7    2.12 372.  ] 1\n",
      "120 [1.20e+01 3.43e+00 2.00e+00 1.90e+01 8.70e+01 2.00e+00 1.64e+00 3.70e-01\n",
      " 1.87e+00 1.28e+00 9.30e-01 3.05e+00 5.64e+02] 1\n",
      "121 [1.145e+01 2.400e+00 2.420e+00 2.000e+01 9.600e+01 2.900e+00 2.790e+00\n",
      " 3.200e-01 1.830e+00 3.250e+00 8.000e-01 3.390e+00 6.250e+02] 1\n",
      "122 [ 11.56   2.05   3.23  28.5  119.     3.18   5.08   0.47   1.87   6.\n",
      "   0.93   3.69 465.  ] 1\n",
      "123 [ 12.42   4.43   2.73  26.5  102.     2.2    2.13   0.43   1.71   2.08\n",
      "   0.92   3.12 365.  ] 1\n",
      "124 [1.305e+01 5.800e+00 2.130e+00 2.150e+01 8.600e+01 2.620e+00 2.650e+00\n",
      " 3.000e-01 2.010e+00 2.600e+00 7.300e-01 3.100e+00 3.800e+02] 1\n",
      "125 [1.187e+01 4.310e+00 2.390e+00 2.100e+01 8.200e+01 2.860e+00 3.030e+00\n",
      " 2.100e-01 2.910e+00 2.800e+00 7.500e-01 3.640e+00 3.800e+02] 1\n",
      "126 [1.207e+01 2.160e+00 2.170e+00 2.100e+01 8.500e+01 2.600e+00 2.650e+00\n",
      " 3.700e-01 1.350e+00 2.760e+00 8.600e-01 3.280e+00 3.780e+02] 1\n",
      "127 [ 12.43   1.53   2.29  21.5   86.     2.74   3.15   0.39   1.77   3.94\n",
      "   0.69   2.84 352.  ] 1\n",
      "128 [ 11.79   2.13   2.78  28.5   92.     2.13   2.24   0.58   1.76   3.\n",
      "   0.97   2.44 466.  ] 1\n",
      "129 [ 12.37   1.63   2.3   24.5   88.     2.22   2.45   0.4    1.9    2.12\n",
      "   0.89   2.78 342.  ] 1\n",
      "130 [1.204e+01 4.300e+00 2.380e+00 2.200e+01 8.000e+01 2.100e+00 1.750e+00\n",
      " 4.200e-01 1.350e+00 2.600e+00 7.900e-01 2.570e+00 5.800e+02] 1\n",
      "131 [1.286e+01 1.350e+00 2.320e+00 1.800e+01 1.220e+02 1.510e+00 1.250e+00\n",
      " 2.100e-01 9.400e-01 4.100e+00 7.600e-01 1.290e+00 6.300e+02] 2\n",
      "132 [1.288e+01 2.990e+00 2.400e+00 2.000e+01 1.040e+02 1.300e+00 1.220e+00\n",
      " 2.400e-01 8.300e-01 5.400e+00 7.400e-01 1.420e+00 5.300e+02] 2\n",
      "133 [1.281e+01 2.310e+00 2.400e+00 2.400e+01 9.800e+01 1.150e+00 1.090e+00\n",
      " 2.700e-01 8.300e-01 5.700e+00 6.600e-01 1.360e+00 5.600e+02] 2\n",
      "134 [1.27e+01 3.55e+00 2.36e+00 2.15e+01 1.06e+02 1.70e+00 1.20e+00 1.70e-01\n",
      " 8.40e-01 5.00e+00 7.80e-01 1.29e+00 6.00e+02] 2\n",
      "135 [1.251e+01 1.240e+00 2.250e+00 1.750e+01 8.500e+01 2.000e+00 5.800e-01\n",
      " 6.000e-01 1.250e+00 5.450e+00 7.500e-01 1.510e+00 6.500e+02] 2\n",
      "136 [1.26e+01 2.46e+00 2.20e+00 1.85e+01 9.40e+01 1.62e+00 6.60e-01 6.30e-01\n",
      " 9.40e-01 7.10e+00 7.30e-01 1.58e+00 6.95e+02] 2\n",
      "137 [1.225e+01 4.720e+00 2.540e+00 2.100e+01 8.900e+01 1.380e+00 4.700e-01\n",
      " 5.300e-01 8.000e-01 3.850e+00 7.500e-01 1.270e+00 7.200e+02] 2\n",
      "138 [ 12.53   5.51   2.64  25.    96.     1.79   0.6    0.63   1.1    5.\n",
      "   0.82   1.69 515.  ] 2\n",
      "139 [1.349e+01 3.590e+00 2.190e+00 1.950e+01 8.800e+01 1.620e+00 4.800e-01\n",
      " 5.800e-01 8.800e-01 5.700e+00 8.100e-01 1.820e+00 5.800e+02] 2\n",
      "140 [1.284e+01 2.960e+00 2.610e+00 2.400e+01 1.010e+02 2.320e+00 6.000e-01\n",
      " 5.300e-01 8.100e-01 4.920e+00 8.900e-01 2.150e+00 5.900e+02] 2\n",
      "141 [1.293e+01 2.810e+00 2.700e+00 2.100e+01 9.600e+01 1.540e+00 5.000e-01\n",
      " 5.300e-01 7.500e-01 4.600e+00 7.700e-01 2.310e+00 6.000e+02] 2\n",
      "142 [1.336e+01 2.560e+00 2.350e+00 2.000e+01 8.900e+01 1.400e+00 5.000e-01\n",
      " 3.700e-01 6.400e-01 5.600e+00 7.000e-01 2.470e+00 7.800e+02] 2\n",
      "143 [1.352e+01 3.170e+00 2.720e+00 2.350e+01 9.700e+01 1.550e+00 5.200e-01\n",
      " 5.000e-01 5.500e-01 4.350e+00 8.900e-01 2.060e+00 5.200e+02] 2\n",
      "144 [1.362e+01 4.950e+00 2.350e+00 2.000e+01 9.200e+01 2.000e+00 8.000e-01\n",
      " 4.700e-01 1.020e+00 4.400e+00 9.100e-01 2.050e+00 5.500e+02] 2\n",
      "145 [1.225e+01 3.880e+00 2.200e+00 1.850e+01 1.120e+02 1.380e+00 7.800e-01\n",
      " 2.900e-01 1.140e+00 8.210e+00 6.500e-01 2.000e+00 8.550e+02] 2\n",
      "146 [1.316e+01 3.570e+00 2.150e+00 2.100e+01 1.020e+02 1.500e+00 5.500e-01\n",
      " 4.300e-01 1.300e+00 4.000e+00 6.000e-01 1.680e+00 8.300e+02] 2\n",
      "147 [1.388e+01 5.040e+00 2.230e+00 2.000e+01 8.000e+01 9.800e-01 3.400e-01\n",
      " 4.000e-01 6.800e-01 4.900e+00 5.800e-01 1.330e+00 4.150e+02] 2\n",
      "148 [1.287e+01 4.610e+00 2.480e+00 2.150e+01 8.600e+01 1.700e+00 6.500e-01\n",
      " 4.700e-01 8.600e-01 7.650e+00 5.400e-01 1.860e+00 6.250e+02] 2\n",
      "149 [1.332e+01 3.240e+00 2.380e+00 2.150e+01 9.200e+01 1.930e+00 7.600e-01\n",
      " 4.500e-01 1.250e+00 8.420e+00 5.500e-01 1.620e+00 6.500e+02] 2\n",
      "150 [1.308e+01 3.900e+00 2.360e+00 2.150e+01 1.130e+02 1.410e+00 1.390e+00\n",
      " 3.400e-01 1.140e+00 9.400e+00 5.700e-01 1.330e+00 5.500e+02] 2\n",
      "151 [1.35e+01 3.12e+00 2.62e+00 2.40e+01 1.23e+02 1.40e+00 1.57e+00 2.20e-01\n",
      " 1.25e+00 8.60e+00 5.90e-01 1.30e+00 5.00e+02] 2\n",
      "152 [1.279e+01 2.670e+00 2.480e+00 2.200e+01 1.120e+02 1.480e+00 1.360e+00\n",
      " 2.400e-01 1.260e+00 1.080e+01 4.800e-01 1.470e+00 4.800e+02] 2\n",
      "153 [1.311e+01 1.900e+00 2.750e+00 2.550e+01 1.160e+02 2.200e+00 1.280e+00\n",
      " 2.600e-01 1.560e+00 7.100e+00 6.100e-01 1.330e+00 4.250e+02] 2\n",
      "154 [1.323e+01 3.300e+00 2.280e+00 1.850e+01 9.800e+01 1.800e+00 8.300e-01\n",
      " 6.100e-01 1.870e+00 1.052e+01 5.600e-01 1.510e+00 6.750e+02] 2\n",
      "155 [1.258e+01 1.290e+00 2.100e+00 2.000e+01 1.030e+02 1.480e+00 5.800e-01\n",
      " 5.300e-01 1.400e+00 7.600e+00 5.800e-01 1.550e+00 6.400e+02] 2\n",
      "156 [1.317e+01 5.190e+00 2.320e+00 2.200e+01 9.300e+01 1.740e+00 6.300e-01\n",
      " 6.100e-01 1.550e+00 7.900e+00 6.000e-01 1.480e+00 7.250e+02] 2\n",
      "157 [ 13.84   4.12   2.38  19.5   89.     1.8    0.83   0.48   1.56   9.01\n",
      "   0.57   1.64 480.  ] 2\n",
      "158 [1.245e+01 3.030e+00 2.640e+00 2.700e+01 9.700e+01 1.900e+00 5.800e-01\n",
      " 6.300e-01 1.140e+00 7.500e+00 6.700e-01 1.730e+00 8.800e+02] 2\n",
      "159 [1.434e+01 1.680e+00 2.700e+00 2.500e+01 9.800e+01 2.800e+00 1.310e+00\n",
      " 5.300e-01 2.700e+00 1.300e+01 5.700e-01 1.960e+00 6.600e+02] 2\n",
      "160 [1.348e+01 1.670e+00 2.640e+00 2.250e+01 8.900e+01 2.600e+00 1.100e+00\n",
      " 5.200e-01 2.290e+00 1.175e+01 5.700e-01 1.780e+00 6.200e+02] 2\n",
      "161 [1.236e+01 3.830e+00 2.380e+00 2.100e+01 8.800e+01 2.300e+00 9.200e-01\n",
      " 5.000e-01 1.040e+00 7.650e+00 5.600e-01 1.580e+00 5.200e+02] 2\n",
      "162 [1.369e+01 3.260e+00 2.540e+00 2.000e+01 1.070e+02 1.830e+00 5.600e-01\n",
      " 5.000e-01 8.000e-01 5.880e+00 9.600e-01 1.820e+00 6.800e+02] 2\n",
      "163 [ 12.85   3.27   2.58  22.   106.     1.65   0.6    0.6    0.96   5.58\n",
      "   0.87   2.11 570.  ] 2\n",
      "164 [1.296e+01 3.450e+00 2.350e+00 1.850e+01 1.060e+02 1.390e+00 7.000e-01\n",
      " 4.000e-01 9.400e-01 5.280e+00 6.800e-01 1.750e+00 6.750e+02] 2\n",
      "165 [1.378e+01 2.760e+00 2.300e+00 2.200e+01 9.000e+01 1.350e+00 6.800e-01\n",
      " 4.100e-01 1.030e+00 9.580e+00 7.000e-01 1.680e+00 6.150e+02] 2\n",
      "166 [1.373e+01 4.360e+00 2.260e+00 2.250e+01 8.800e+01 1.280e+00 4.700e-01\n",
      " 5.200e-01 1.150e+00 6.620e+00 7.800e-01 1.750e+00 5.200e+02] 2\n",
      "167 [1.345e+01 3.700e+00 2.600e+00 2.300e+01 1.110e+02 1.700e+00 9.200e-01\n",
      " 4.300e-01 1.460e+00 1.068e+01 8.500e-01 1.560e+00 6.950e+02] 2\n",
      "168 [1.282e+01 3.370e+00 2.300e+00 1.950e+01 8.800e+01 1.480e+00 6.600e-01\n",
      " 4.000e-01 9.700e-01 1.026e+01 7.200e-01 1.750e+00 6.850e+02] 2\n",
      "169 [1.358e+01 2.580e+00 2.690e+00 2.450e+01 1.050e+02 1.550e+00 8.400e-01\n",
      " 3.900e-01 1.540e+00 8.660e+00 7.400e-01 1.800e+00 7.500e+02] 2\n",
      "170 [1.34e+01 4.60e+00 2.86e+00 2.50e+01 1.12e+02 1.98e+00 9.60e-01 2.70e-01\n",
      " 1.11e+00 8.50e+00 6.70e-01 1.92e+00 6.30e+02] 2\n",
      "171 [1.22e+01 3.03e+00 2.32e+00 1.90e+01 9.60e+01 1.25e+00 4.90e-01 4.00e-01\n",
      " 7.30e-01 5.50e+00 6.60e-01 1.83e+00 5.10e+02] 2\n",
      "172 [ 12.77       2.39       2.28      19.5       86.         1.39\n",
      "   0.51       0.48       0.64       9.899999   0.57       1.63\n",
      " 470.      ] 2\n",
      "173 [1.416e+01 2.510e+00 2.480e+00 2.000e+01 9.100e+01 1.680e+00 7.000e-01\n",
      " 4.400e-01 1.240e+00 9.700e+00 6.200e-01 1.710e+00 6.600e+02] 2\n",
      "174 [1.371e+01 5.650e+00 2.450e+00 2.050e+01 9.500e+01 1.680e+00 6.100e-01\n",
      " 5.200e-01 1.060e+00 7.700e+00 6.400e-01 1.740e+00 7.400e+02] 2\n",
      "175 [1.34e+01 3.91e+00 2.48e+00 2.30e+01 1.02e+02 1.80e+00 7.50e-01 4.30e-01\n",
      " 1.41e+00 7.30e+00 7.00e-01 1.56e+00 7.50e+02] 2\n",
      "176 [1.327e+01 4.280e+00 2.260e+00 2.000e+01 1.200e+02 1.590e+00 6.900e-01\n",
      " 4.300e-01 1.350e+00 1.020e+01 5.900e-01 1.560e+00 8.350e+02] 2\n",
      "177 [1.317e+01 2.590e+00 2.370e+00 2.000e+01 1.200e+02 1.650e+00 6.800e-01\n",
      " 5.300e-01 1.460e+00 9.300e+00 6.000e-01 1.620e+00 8.400e+02] 2\n",
      "178 [ 14.13   4.1    2.74  24.5   96.     2.05   0.76   0.56   1.35   9.2\n",
      "   0.61   1.6  560.  ] 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(wine.data)):\n",
    "  print(i+1, wine.data[i], wine.target[i]) # sample -> [feature vector], label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  class  \n",
       "0                            3.92   1065.0      0  \n",
       "1                            3.40   1050.0      0  \n",
       "2                            3.17   1185.0      0  \n",
       "3                            3.45   1480.0      0  \n",
       "4                            2.93    735.0      0  \n",
       "..                            ...      ...    ...  \n",
       "173                          1.74    740.0      2  \n",
       "174                          1.56    750.0      2  \n",
       "175                          1.56    835.0      2  \n",
       "176                          1.62    840.0      2  \n",
       "177                          1.60    560.0      2  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(wine.data, columns=wine.feature_names) # feature\n",
    "df['class'] = wine.target # target\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium',\n",
       "       'total_phenols', 'flavanoids', 'nonflavanoid_phenols',\n",
       "       'proanthocyanins', 'color_intensity', 'hue',\n",
       "       'od280/od315_of_diluted_wines', 'proline', 'class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[wine.feature_names]\n",
    "y = df['class'] # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tn, X_te, y_tn, y_te = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler()\n",
    "std_scale.fit(X_tn)\n",
    "X_tn_std = std_scale.transform(X_tn)\n",
    "X_te_std = std_scale.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0]\n",
      " [ 0 21  0]\n",
      " [ 0  0  8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Support Vector Machine\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "clf_svm.fit(X_tn_std, y_tn) # train\n",
    "\n",
    "pred_svm = clf_svm.predict(X_te_std) # task\n",
    "\n",
    "conf_matrix = confusion_matrix(y_te, pred_svm)\n",
    "print(conf_matrix)\n",
    "\n",
    "class_report = classification_report(y_te, pred_svm)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0]\n",
      " [ 0 21  0]\n",
      " [ 0  0  8]]\n",
      "[[16.  0.  0.]\n",
      " [ 0. 21.  0.]\n",
      " [ 0.  0.  8.]]\n",
      "The accuracy of test set is: 100.0 %.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Support Vector Machine\n",
    "clf_svm = svm.SVC(gamma=0.1, C=10)\n",
    "clf_svm.fit(X_tn_std, y_tn)\n",
    "\n",
    "pred_svm = clf_svm.predict(X_te_std)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_te, pred_svm)\n",
    "print(conf_matrix)\n",
    "\n",
    "# confusion matrix ì§ì ‘ êµ¬í˜„ ì—°ìŠµ\n",
    "conf_practice = np.zeros((3, 3))\n",
    "for i in range(len(pred_svm)):\n",
    "  conf_practice[pred_svm[i]][y_te.tolist()[i]] += 1\n",
    "print(conf_practice)\n",
    "\n",
    "wrong = 0\n",
    "for i in range(3):\n",
    "  wrong += conf_practice[i][i]\n",
    "accuracy = wrong / len(pred_svm)\n",
    "print(\"The accuracy of test set is:\", accuracy*100, \"%.\")\n",
    "\n",
    "\n",
    "class_report = classification_report(y_te, pred_svm)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66666667 0.63888889 0.63888889 0.65714286 0.8       ]\n",
      "accuracy(avg)=68.032, standard deviation=0.061\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import datasets\n",
    "\n",
    "wine = datasets.load_wine()\n",
    "S = svm.SVC(gamma=0.001)\n",
    "accuracies = cross_val_score(S, wine.data, wine.target, cv=5) # 5-fold\n",
    "\n",
    "print(accuracies)\n",
    "print(\"accuracy(avg)=%0.3f, standard deviation=%0.3f\" %(accuracies.mean()*100, accuracies.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0]\n",
      " [ 0 21  0]\n",
      " [ 0  0  8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(X_tn_std, y_tn)\n",
    "\n",
    "pred_lr = clf_lr.predict(X_te_std)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_te, pred_lr)\n",
    "print(conf_matrix)\n",
    "\n",
    "class_report = classification_report(y_te, pred_lr)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0]\n",
      " [ 2 18  1]\n",
      " [ 0  0  8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94        16\n",
      "           1       1.00      0.86      0.92        21\n",
      "           2       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.95      0.94        45\n",
      "weighted avg       0.94      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "clf_gnb = GaussianNB()\n",
    "clf_gnb.fit(X_tn_std, y_tn)\n",
    "\n",
    "pred_gnb = clf_gnb.predict(X_te_std)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_te, pred_gnb)\n",
    "print(conf_matrix)\n",
    "\n",
    "class_report = classification_report(y_te, pred_gnb)\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hira",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
